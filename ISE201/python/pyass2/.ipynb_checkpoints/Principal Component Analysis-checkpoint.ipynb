{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis using the Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "PCA is used to explain the variance-covariance structure of the attributes or variables or features in a dataset using a few linear combinations of those variables. This analysis serves as an intermediate step to much large objectives such as classification or regression analysis or cluster analysis or factor analysis. \n",
    "\n",
    "This assignment walks through doing a principal component analysis using numpy library functions. There are many other libraries in Python that might achieve the same , one such instance is provided towards the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default in a Jupyter notebook, a cell with multiple print commands, when run, would print only the last one. \n",
    "# This piece of code would modify that to print all the relevant lines in the cell.  \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "To illustrate PCA we will use the Breast Cancer data available here https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/\n",
    "\n",
    "The data was collected in 1995 and has 569 observations and each observation has 30 features or attributes. There are no missing attribute values. There are two classes : Malignant (38%) and Benign ( 62%).\n",
    "\n",
    "You can read more about the data https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "For this analysis, the files used are wdbc.data (and wdbc.names for the column headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and quickly assess if it was loaded correctly by printing the first/last few rows\n",
    "\n",
    "missing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n/a\", \"na\", \"--\", \" \"]\n",
    "colnames = ['ID', 'Diagnosis', 'mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
    "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
    "       'smoothness error', 'compactness error', 'concavity error',\n",
    "       'concave points error', 'symmetry error',\n",
    "       'fractal dimension error', 'worst radius', 'worst texture',\n",
    "       'worst perimeter', 'worst area', 'worst smoothness',\n",
    "       'worst compactness', 'worst concavity', 'worst concave points',\n",
    "       'worst symmetry', 'worst fractal dimension']\n",
    "df_raw = pd.read_csv('wdbc.data', header = None, names = colnames, na_values = missing_value_formats)\n",
    "df_raw.shape\n",
    "df_raw.head(5)\n",
    "df_raw.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "In case there are missing values, outliers, format mismatches, erroneous observations, additional attributes not relevant for the analysis etc., we might have to redo this section multiple times till we achieve a level of condifence in the accuracy of the dataset. This dataset does not have any missing values or anomolous observations of concern.\n",
    "\n",
    "Lets compute summary statistics and information on data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "This section is to illustrate *some*  of the data visualization techniques that might help us to judge if PCA would be helpful for this data set or not. \n",
    "\n",
    "The first is to check if the 30 or so attributes have visual variability between the two categories of the cancer. This is a quick check since this data set has been used for classification of tumor into categories malignant or benign, using features of the tumor. So we would want to check if there are differences in the attributes for the two categories (we will talk more about it during Regression Analysis later in the course). \n",
    "\n",
    "The second is to check if there are correlations between the attributes or variables. If the variables are independent (aka, correlation is 0 or close to 0), principal components would just be the attributes and would offer no dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html\n",
    "# Please note the hardcoded values \"-2\" in the range and \"+2\" in the indexing the colnames are to ensure \n",
    "# that we do not the plot for the first two columns , ID and Diagnosis\n",
    "\n",
    "for i in range(df_raw.shape[1]-2):\n",
    "    df_raw.boxplot(column=colnames[i+2] , by=['Diagnosis'], figsize=(12,9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html\n",
    "\n",
    "# Not keen on seeing the scatter plot for ID and Diagnosis\n",
    "scatter_matrix(df_raw.iloc[:,2:32], alpha=0.9, figsize=(50, 50), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "Seems like quite a few variables may have variability to differentiate between the two categories and many of them are correlated. There is a scope for reducing the dimension of the attribute set from 30 to a smaller number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "Since PCA is an unsupervised approach (the analysis does not depend on the labels or categories or response to predict), we would want to remove the ID and the Diagnosis columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.iloc[:,2:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "The analysis consists of five main steps:\n",
    "1. Computing the covariance matrix. Covariance matrix is a square symmetric matrix that captures the variance of the attributes on the diagonal and pair-wise covariances among attributes on the off diagonal. When the covariance values are scaled by the variance on the diagonal and product of the individual standard deviations on the off diagnoal, the resultant matrix is known as Correlation matrix. It is also same as computing the covariance of standardized (centered and scaled, i.e. subtract mean and divide by standard deviation) data matrix\n",
    "2. Factoring the covariance (or correlation) matrix into eigenvalues and eigenvectors\n",
    "3. Scree plot. It is a plot of eigenvalue order number vs proportion of variance explained by the eigenvalue. It is a useful visual way to to determine how many components should be selected. To determine the number of compnents we look for an bend in the scree plot.\n",
    "4. Computing the principal components  \n",
    "5. Interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov = df.cov()\n",
    "df_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://numpy.org/doc/stable/reference/routines.linalg.html\n",
    "    \n",
    "w, v = np.linalg.eig(df_cov)\n",
    "\n",
    "print(w)\n",
    "#print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PVE = w/w.sum()\n",
    "PVE\n",
    "np.cumsum(PVE)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(PVE, linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Principal Component\n",
    "\n",
    "(0.00929)*mean radius + (-0.00288)*mean texture + (0.06275)*mean perimeter + (0.85182)*mean area + (-0.00001)*mean smoothness + (-0.00000)*mean compactness + (0.00008)*mean concavity + (0.00005)*mean concave points + (-0.00003)*mean symmetry + (-0.00002)*mean fractal dimension + (-0.00005)*radius error + (0.00035)*texture error + (0.00082)*perimeter error + (0.00751)*area error + (0.00000)*smoothness error + (0.00001)*compactness error + (0.00003)*concavity error + (0.00001)*concave points error + (0.00001)*symmetry error + (0.00000)*fractal dimension error + (-0.00057)*worst radius + (-0.01322)*worst texture + (-0.00019)*worst perimeter + (-0.51974)*worst area + (-0.00008)*worst smoothness + (-0.00026)*worst compactness + (-0.00018)*worst concavity + (-0.00003)*worst concave points + (-0.00016)*worst symmetry + (-0.00006)*worst fractal dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "From the Scree plot (eigenvalue vs PVE), we can visually deduce that the first component would capture about 98% of the variability, but when we take a closer look at the first principal component, we quickly realize it is dominated by variables with large variances and magnitudes, 'mean area' and 'worst area'. We can correct this by considering the correlation matrix for computing the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix (or Covariance matrix of Standardized Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_, v_ = np.linalg.eig(df_corr)\n",
    "\n",
    "print(w_)\n",
    "#print(v_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determing the number of principal components\n",
    "PVE_ = w_/w_.sum()\n",
    "PVE_\n",
    "np.cumsum(PVE_)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(PVE_, linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(v[:,1:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First few principal components\n",
    "First Principal Component:\n",
    "(-0.23386)*mean radius + (-0.05971)*mean texture + (-0.21518)*mean perimeter + (-0.23108)*mean area + (0.18611)*mean smoothness + (0.15189)*mean compactness + (0.06017)*mean concavity + (-0.03477)*mean concave points + (0.19035)*mean symmetry + (0.36658)*mean fractal dimension + (-0.10555)*radius error + (0.08998)*texture error + (-0.08946)*perimeter error + (-0.15229)*area error + (0.20443)*smoothness error + (0.23272)*compactness error + (0.19721)*concavity error + (0.13032)*concave points error + (0.18385)*symmetry error + (0.28009)*fractal dimension error + (-0.21987)*worst radius + (-0.04547)*worst texture + (-0.19988)*worst perimeter + (-0.21935)*worst area + (0.17230)*worst smoothness + (0.14359)*worst compactness + (0.09796)*worst concavity + (-0.00826)*worst concave points + (0.14188)*worst symmetry + (0.27534)*worst fractal dimension\n",
    "\n",
    "Second Principal Component:\n",
    "(-0.00853)*mean radius + (0.06455)*mean texture + (-0.00931)*mean perimeter + (0.02870)*mean area + (-0.10429)*mean smoothness + (-0.07409)*mean compactness + (0.00273)*mean concavity + (-0.02556)*mean concave points + (-0.04024)*mean symmetry + (-0.02257)*mean fractal dimension + (0.26848)*radius error + (0.37463)*texture error + (0.26665)*perimeter error + (0.21601)*area error + (0.30884)*smoothness error + (0.15478)*compactness error + (0.17646)*concavity error + (0.22466)*concave points error + (0.28858)*symmetry error + (0.21150)*fractal dimension error + (-0.04751)*worst radius + (-0.04230)*worst texture + (-0.04855)*worst perimeter + (-0.01190)*worst area + (-0.25980)*worst smoothness + (-0.23608)*worst compactness + (-0.17306)*worst concavity + (-0.17034)*worst concave points + (-0.27131)*worst symmetry + (-0.23279)*worst fractal dimension\n",
    "\n",
    "Third Principal Component:\n",
    "(0.04141)*mean radius + (-0.60305)*mean texture + (0.04198)*mean perimeter + (0.05343)*mean area + (0.15938)*mean smoothness + (0.03179)*mean compactness + (0.01912)*mean concavity + (0.06534)*mean concave points + (0.06712)*mean symmetry + (0.04859)*mean fractal dimension + (0.09794)*radius error + (-0.35986)*texture error + (0.08899)*perimeter error + (0.10821)*area error + (0.04466)*smoothness error + (-0.02747)*compactness error + (0.00132)*concavity error + (0.07407)*concave points error + (0.04407)*symmetry error + (0.01530)*fractal dimension error + (0.01542)*worst radius + (-0.63281)*worst texture + (0.01380)*worst perimeter + (0.02589)*worst area + (0.01765)*worst smoothness + (-0.09133)*worst compactness + (-0.07395)*worst concavity + (0.00601)*worst concave points + (-0.03625)*worst symmetry + (-0.07705)*worst fractal dimension\n",
    "\n",
    "Fourth Principal Component:\n",
    "(-0.03779)*mean radius + (0.04947)*mean texture + (-0.03737)*mean perimeter + (-0.01033)*mean area + (0.36509)*mean smoothness + (-0.01170)*mean compactness + (-0.08638)*mean concavity + (0.04386)*mean concave points + (0.30594)*mean symmetry + (0.04442)*mean fractal dimension + (0.15446)*radius error + (0.19165)*texture error + (0.12099)*perimeter error + (0.12757)*area error + (0.23207)*smoothness error + (-0.27997)*compactness error + (-0.35398)*concavity error + (-0.19555)*concave points error + (0.25287)*symmetry error + (-0.26330)*fractal dimension error + (0.00441)*worst radius + (0.09288)*worst texture + (-0.00745)*worst perimeter + (0.02739)*worst area + (0.32444)*worst smoothness + (-0.12180)*worst compactness + (-0.18852)*worst concavity + (-0.04333)*worst concave points + (0.24456)*worst symmetry + (-0.09442)*worst fractal dimension\n",
    "\n",
    "\n",
    "Fifth Principal Component:\n",
    "(0.01874)*mean radius + (-0.03218)*mean texture + (0.01731)*mean perimeter + (-0.00189)*mean area + (-0.28637)*mean smoothness + (-0.01413)*mean compactness + (-0.00934)*mean concavity + (-0.05205)*mean concave points + (0.35646)*mean symmetry + (-0.11943)*mean fractal dimension + (-0.02560)*radius error + (-0.02875)*texture error + (0.00181)*perimeter error + (-0.04286)*area error + (-0.34292)*smoothness error + (0.06920)*compactness error + (0.05634)*concavity error + (-0.03122)*concave points error + (0.49025)*symmetry error + (-0.05320)*fractal dimension error + (-0.00029)*worst radius + (-0.05001)*worst texture + (0.00850)*worst perimeter + (-0.02516)*worst area + (-0.36926)*worst smoothness + (0.04771)*worst compactness + (0.02838)*worst concavity + (-0.03087)*worst concave points + (0.49893)*worst symmetry + (-0.08022)*worst fractal dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "For standardized data, the variables contribute equally to the principal components determined from the correlation matrix. We can successfully replace the original data matrix with the first 14 prinicpal components cumulatively explaining 98% of the total sample variance. Hence we have reduced the dimensions from 30 attributes to 14 with little loss of information.\n",
    "\n",
    "Regarding interpretation, we can deduce by looking at the weights that the first principal seems to be a contrast of attributes capturing the size versus other features such as smoothness, compactness, symmetry etc. The second principal component could be interpreted as weighted difference of \"error\" versus the \"worst\" aspect of the attributes. The third principal component is capturing the \"texture\" related features.  \n",
    "\n",
    "Principal components derived from covariance matrix are different from the ones derived from the correlation matrix. If the original dataset has attrubutes on varying scales, it is recommended to standardized the data matrix or use correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is also available as part of the Scikit-Learn library\n",
    "Scikit-Learn is machine learning library available in python. As part of the library there are seven datasets provided to learn the various approaches. You can read about the different datasets and tutorial on loading them here\n",
    "https://towardsdatascience.com/how-to-use-scikit-learn-datasets-for-machine-learning-d6493b38eca3\n",
    "\n",
    "\n",
    "For additional reading on scikit-learn: https://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = sklearn.datasets.load_breast_cancer()\n",
    "# the object returned is a bunch object , a dictionary like object with keys as attributes\n",
    "cancer.data.shape\n",
    "colnames = cancer.feature_names\n",
    "print(cancer.DESCR)\n",
    "\n",
    "#Convert the format bunch to a pandas dataframe for ease of mani\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(df)\n",
    "df_scaled = pd.DataFrame(scaler.transform(df))\n",
    "\n",
    "#df_scaled.mean()\n",
    "#df_scaled.std()\n",
    "\n",
    "# Standardizing data matrix\n",
    "df_stand = pd.DataFrame(preprocessing.StandardScaler().fit_transform(df))\n",
    "#df_stand.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=30) \n",
    "pca.fit(df_scaled) \n",
    "X_pca=pca.transform(df_scaled) \n",
    "#let's check the shape of X_pca array\n",
    "print(\"shape of X_pca\", X_pca.shape)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
