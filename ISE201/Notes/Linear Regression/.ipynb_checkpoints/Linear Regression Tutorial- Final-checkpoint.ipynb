{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default in a Jupyter notebook, a cell with multiple print commands, when run, would print only the last one. \n",
    "# This piece of code would modify that to print all the relevant lines in the cell.  \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2ba3154a-c2aa-3158-1984-63ad2c0c786a"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import statsmodels.api as sm # import statsmodels \n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Definitions\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "To illustrate regression analysis, we will use AMES data. Data set contains information from the Ames Assessor’s Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010.\n",
    "\n",
    "\n",
    "Source:\n",
    "De Cock, D. (2011). \"Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project,\" Journal of Statistics Education, Volume 19, Number 3.\n",
    "\n",
    "http://jse.amstat.org/v19n3/decock/DataDocumentation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "21fa35be-878b-b4f2-ef6e-68dc070b8bfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indata : (2930, 82)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "indata = pd.read_csv(\"AmesHousing.csv\")\n",
    "print(\"indata : \" + str(indata.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>PID</th>\n",
       "      <th>MS SubClass</th>\n",
       "      <th>MS Zoning</th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Lot Shape</th>\n",
       "      <th>Land Contour</th>\n",
       "      <th>...</th>\n",
       "      <th>Pool Area</th>\n",
       "      <th>Pool QC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>Misc Feature</th>\n",
       "      <th>Misc Val</th>\n",
       "      <th>Mo Sold</th>\n",
       "      <th>Yr Sold</th>\n",
       "      <th>Sale Type</th>\n",
       "      <th>Sale Condition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>526301100</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>141.000</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>526350040</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.000</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>526351010</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.000</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>526353030</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>93.000</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>527105010</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.000</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order        PID  MS SubClass MS Zoning  Lot Frontage  Lot Area Street  \\\n",
       "0      1  526301100           20        RL       141.000     31770   Pave   \n",
       "1      2  526350040           20        RH        80.000     11622   Pave   \n",
       "2      3  526351010           20        RL        81.000     14267   Pave   \n",
       "3      4  526353030           20        RL        93.000     11160   Pave   \n",
       "4      5  527105010           60        RL        74.000     13830   Pave   \n",
       "\n",
       "  Alley Lot Shape Land Contour  ... Pool Area Pool QC  Fence Misc Feature  \\\n",
       "0   NaN       IR1          Lvl  ...         0     NaN    NaN          NaN   \n",
       "1   NaN       Reg          Lvl  ...         0     NaN  MnPrv          NaN   \n",
       "2   NaN       IR1          Lvl  ...         0     NaN    NaN         Gar2   \n",
       "3   NaN       Reg          Lvl  ...         0     NaN    NaN          NaN   \n",
       "4   NaN       IR1          Lvl  ...         0     NaN  MnPrv          NaN   \n",
       "\n",
       "  Misc Val Mo Sold Yr Sold Sale Type  Sale Condition  SalePrice  \n",
       "0        0       5    2010       WD           Normal     215000  \n",
       "1        0       6    2010       WD           Normal     105000  \n",
       "2    12500       6    2010       WD           Normal     172000  \n",
       "3        0       4    2010       WD           Normal     244000  \n",
       "4        0       3    2010       WD           Normal     189900  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Creation\n",
    "Skipped in this notebook are all the steps for data cleaning and feature creation\n",
    "Please refer to this notebook for a detailed insight into the above mentioned aspects\n",
    "https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Random split\n",
    "#train, test = train_test_split(indata, train_size=0.70, random_state=42)\n",
    "#print(\"train : \" + str(train.shape))\n",
    "#print(\"test : \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by year\n",
    "train = indata[np.array(indata['Yr Sold'].astype(np.float32)) < 2010]\n",
    "test = indata[np.array(indata['Yr Sold'].astype(np.float32)) >= 2010]\n",
    "print(\"train : \" + str(train.shape))\n",
    "print(\"test : \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a345817d-8b0c-7467-7524-16a4cbb02794"
   },
   "outputs": [],
   "source": [
    "# Find most important features relative to target\n",
    "print(\"Find most important features relative to target\")\n",
    "corr = train.corr()\n",
    "corr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\n",
    "print(corr.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slm = train[['Gr Liv Area','SalePrice']]\n",
    "X = train_slm[['Gr Liv Area']] ## X usually means our input variables (or independent variables)\n",
    "y = train_slm.SalePrice ## Y usually means our output/dependent variable\n",
    "\n",
    "test_slm = test[['Gr Liv Area','SalePrice']]\n",
    "X_test = test_slm[['Gr Liv Area']]\n",
    "y_test = test_slm.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_slm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot of response and regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9e99565-3296-cb48-3cc7-722b94e3fdac"
   },
   "outputs": [],
   "source": [
    "# Lets fit a simple linear model using a single regressor \n",
    "plt.scatter(X, y, c = \"blue\", marker = \"s\")\n",
    "plt.title(\"Bi-variate Scatter Plot\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Influencial observations\n",
    "train_slm[np.array(train_slm['Gr Liv Area'].astype(np.float32)) > 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Remove influential observations\n",
    "train_cl = train_slm[np.array(train_slm['Gr Liv Area'].astype(np.float32)) < 4000]\n",
    "print(\"num of observations removed : \" + str(train_slm.shape[0] - train_cl.shape[0]))\n",
    "X = train_cl[['Gr Liv Area']] ## X usually means our input variables (or independent variables)\n",
    "y = train_cl.SalePrice ## Y usually means our output/dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Analysis of Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Probability plot of the Response\n",
    "from scipy import stats\n",
    "stats.probplot(y, plot=plt)\n",
    "plt.show()\n",
    "\n",
    "#Normal Probability plot of transformed response\n",
    "from scipy import stats\n",
    "stats.probplot(np.log(y), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_= np.log(y)\n",
    "y_test_ = np.log(y_test)\n",
    "#y_= y\n",
    "#y_test_=y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = sm.add_constant(X) ## let's add an intercept (beta_0) to our model\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_, X_).fit() ## sm.OLS(output, input)\n",
    "predictions = model.predict(X_)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters: ', model.params)\n",
    "print('Standard errors: ', model.bse)\n",
    "print('Predicted values: ', model.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals vs Regressor\n",
    "residuals = y_- predictions\n",
    "plt.plot(X,residuals, 'o', color='darkblue')\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Independent Variable\")\n",
    "plt.ylabel(\"Residual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residuals vs Predicted\n",
    "residuals = y_ - predictions\n",
    "plt.plot(predictions,residuals, 'o', color='darkblue')\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.xlabel(\"Predicted response\")\n",
    "plt.ylabel(\"Residual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Probability Plot of residuals\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Our Actual and Predicted Values\n",
    "plt.plot(X, y_, 'o', color='black');\n",
    "plt.plot(X,predictions,color='blue')\n",
    "plt.title(\"Actuals vs Regression Line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test and CI on the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#tStat, pValue =  scipy.stats.ttest_1samp(model.params[1], 0, axis=0)\n",
    "#print(\"P-Value:{0} T-Statistic:{1}\".format(pValue,tStat))\n",
    "N = 2589\n",
    "t = model.params[1]/model.bse[1]\n",
    "print(\"t-statistic for slope : \" + str(t))\n",
    "\n",
    "## Compare with the critical t-value\n",
    "#Degrees of freedom\n",
    "df = N - 2\n",
    "\n",
    "#p-value after comparison with the t \n",
    "p = 1 - stats.t.cdf(t,df=df)\n",
    "print(\"p-value for hypothesis on slope=0 : \" + str(p))\n",
    "\n",
    "#Confidence Interval on the slope\n",
    "ci_slope = stats.t.interval(alpha=0.95, df=N-1, loc=model.params[1], scale=model.bse[1]) \n",
    "print(\"CI on the slope parameter : \" + str(ci_slope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(sm.add_constant(X_test))\n",
    "predictions.head()\n",
    "y_test_.head()\n",
    "rmse = math.sqrt(np.square(np.subtract(y_test_,predictions)).mean())\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Adequacy Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-test\n",
    "print(model.t_test([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anova or F test\n",
    "print(model.f_test(np.identity(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence and Leverage points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the influence of each observation\n",
    "test_class = OLSInfluence(model)\n",
    "test_class.dfbetas[(1063,2180),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for leverage and influential outliers\n",
    "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "fig = plot_leverage_resid2(model, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Hat Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing inverse Hat matrix X((X'X)^-1)X'\n",
    "print(\"X : \" + str(X_.shape))\n",
    "Cmat = np.linalg.inv(X_.transpose().dot(X_))\n",
    "#(X'X)^-1\n",
    "print(\"C matrix or (X'X)^-1 : \" + str(Cmat.shape))\n",
    "hat_mat_ = Cmat.dot(X_.transpose())\n",
    "hat_mat = X_.dot(hat_mat_)\n",
    "print(\"Hat matrix or X((X'X)^-1)X' : \" + str(hat_mat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lm = linear_model.LinearRegression(fit_intercept = True, normalize = True)\n",
    "model = lm.fit(X,y_)\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "#Returns the coefficient of determination R^2 of the prediction.\n",
    "lm.score(X,y_)\n",
    "\n",
    "print(\"slope: {0}\".format(lm.coef_))\n",
    "print(\"intercept: {0}\".format(lm.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "MSE = mean_squared_error(y_test, predictions)\n",
    " \n",
    "RMSE = math.sqrt(MSE)\n",
    "print(\"Root Mean Square Error: \" + RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slm = train[['Overall Qual','SalePrice']]\n",
    "X = train_slm[['Overall Qual']] ## X usually means our input variables (or independent variables)\n",
    "y = train_slm.SalePrice ## Y usually means our output/dependent variable\n",
    "\n",
    "test_slm = test[['Overall Qual','SalePrice']]\n",
    "X_test = test_slm[['Overall Qual']]\n",
    "y_test = test_slm.SalePrice\n",
    "\n",
    "y_= np.log(y)\n",
    "y_test_ = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qual_ind = pd.get_dummies(X['Overall Qual'], drop_first=True).rename(columns=lambda x: 'OverallQual_' + str(x))\n",
    "\n",
    "X_ind = pd.concat([X, Qual_ind], axis=1)\n",
    "X_ind.head()\n",
    "\n",
    "result = sm.OLS(y_, sm.add_constant(Qual_ind)).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indata.shape\n",
    "Qual_ind = pd.get_dummies(indata['Overall Qual'], drop_first=True).rename(columns=lambda x: 'OverallQual_' + str(x))\n",
    "indata_mod = pd.concat([indata, Qual_ind], axis=1)\n",
    "indata_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by year\n",
    "train = indata_mod[np.array(indata_mod['Yr Sold'].astype(np.float32)) < 2010]\n",
    "test = indata_mod[np.array(indata_mod['Yr Sold'].astype(np.float32)) >= 2010]\n",
    "print(\"train : \" + str(train.shape))\n",
    "print(\"test : \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slm = train[['OverallQual_2','OverallQual_3','OverallQual_4','OverallQual_5','OverallQual_6','OverallQual_7', 'OverallQual_8','OverallQual_9','OverallQual_10','Gr Liv Area','SalePrice']]\n",
    "X = train_slm[['Gr Liv Area','OverallQual_2','OverallQual_3','OverallQual_4','OverallQual_5','OverallQual_6','OverallQual_7', 'OverallQual_8','OverallQual_9','OverallQual_10']] ## X usually means our input variables (or independent variables)\n",
    "y = train_slm.SalePrice ## Y usually means our output/dependent variable\n",
    "\n",
    "y_=np.log2(y)\n",
    "\n",
    "test_slm = test[['OverallQual_2','OverallQual_3','OverallQual_4','OverallQual_5','OverallQual_6','OverallQual_7', 'OverallQual_8','OverallQual_9','OverallQual_10','Gr Liv Area','SalePrice']]\n",
    "X_test = test_slm[['Gr Liv Area','OverallQual_2','OverallQual_3','OverallQual_4','OverallQual_5','OverallQual_6','OverallQual_7', 'OverallQual_8','OverallQual_9','OverallQual_10']]\n",
    "y_test = test_slm.SalePrice\n",
    "\n",
    "y_test_ = np.log2(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.OLS(y_, sm.add_constant(X)).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = result.predict(sm.add_constant(X_test))\n",
    "predictions.head()\n",
    "y_test_.head()\n",
    "rmse = math.sqrt(np.square(np.subtract(y_test_,predictions)).mean())\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Analysis on Ames data highlighting cleaning and feature creation https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
